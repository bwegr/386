---
layout: post
title:  "BYU MBA Slack Data - EDA"
author: Brandon Wegrowski
description: "Cleaning up data from the BYU MBA Slack Workspace"
image: "/assets/images/slack2.webp"
--- 

#### **Introduction**
This is Part 2 of my project exploring BYU MBA Slack data. Now that our data is cleaned up and prepared for EDA, we will dive in and attempt to answer a few questions that will, hopefully, uncover some interesting insights about communication patterns and informal social networks in the BYU MBA program. Those questions are:
- Who reacts to messages most?
- What are the most popular emojis?
- Who generates the most reactions (total and per message)?
- Who replies to messages most?
- Who generates the most replies (total and per message)?
- Who generates the fastest replies?
- What kind of networks exist within channel membership?

#### **Question 1: Who reacts to messages most?**

To answer this question (see code below) we will grab a subset of our main data frame that only includes the 'user' and 'reactions' columns. Each record in the 'reactions' column contains a string of nested JSON, so we will use a function with a regular expression to extract the 'count' value (the number of reactions) into a new column. We will then create another function that will split things out of that column in a coherent way into a dictionary of lists. Then we simply take that lit of dictionaries and concatenate them together into a single data frame and merge that with the 'users' data frame to get the real names of people. With clean, isolated data on reactions, we can now make a bar graph with that data frame that shows us the top ten reactors.

```python
import re
import ast
from pandas import json_normalize
import matplotlib.pyplot as plt

react1 = pd.DataFrame(data[data['reactions'].notna()][['user','reactions']])

def num_and_sum(s):
    pattern = r"'count': (\d+)"
    counts = [int(num) for num in re.findall(pattern, s)]
    return sum(counts)

react1['sum_counts'] = react1['reactions'].astype(str).apply(num_and_sum)
react1a = react1.groupby('user')['sum_counts'].sum().reset_index()

def convert_string_to_dict(s):
    data = ast.literal_eval(s)
    result = {'name': [], 'users': [], 'count': []}
    
    for item in data:
        result['name'].extend([item['name']] * item['count'])
        result['users'].extend(item['users'])
        result['count'].extend([item['count']] * item['count'])
    
    return result

list_of_dic = react1['reactions'].astype(str).apply(convert_string_to_dict).tolist()

# Concatenate all data frames into one
react2 = pd.DataFrame({'name': [], 'users': [], 'count': []})
for i in list_of_dic:
    max_length = max(len(lst) for lst in i.values())
    for key in i:
        i[key].extend([None] * (max_length - len(i[key])))
    df = pd.DataFrame(i)
    react2 = pd.concat([react2, df], ignore_index=True)

# Join users table to get real names for users
react2b = pd.merge(react2, users[['id', 'real_name']], left_on='users', right_on = 'id', how='left')
react2b

# Visualize
react2a = react2b[['real_name', 'count']].groupby('real_name').count().reset_index().sort_values(by='count', ascending=False)
paired = sorted(zip(react2a['count'][0:10], react2a['real_name'][0:10]))
count_s, users_s = zip(*paired)
plt.barh(users_s, count_s)
for index, value in enumerate(count_s):
    plt.text(value-12, index, str(value), va='center')
plt.xlabel('Messages Reacted To')
plt.ylabel('Name')
plt.title('Top Ten Reactors (Oct 2023)')
plt.show()
```
<html>
<!-- <style>
.center {display: block; margin-left: auto; margin-right: auto;
  width: 50%; /* Adjust as needed */
}
</style> -->
    <img src="https://raw.githubusercontent.com/bwegr/386/main/assets/images/topreactors.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width:500px;"/><br><br></html>

#### **Question 2: What are the most popular emojis?**

We can answer now more questions about reactions like "What are the most popular emojis?". We will use the same 'react2' data frame that we just created. The top ten are dominated by positive reactions. We can also see that the trend here is less linear and more curved, which suggests that one person's emoji use may influence other people's emoji use.

```python
react2c = react2[['name', 'count']].groupby('name').count().reset_index().sort_values(by='count', ascending=False)
paired = sorted(zip(react2c['count'][0:10], react2c['name'][0:10]))
count_s, emoji_s = zip(*paired)
plt.barh(emoji_s, count_s)
for index, value in enumerate(count_s):
    plt.text(value-55, index, str(value), va='center')
#plt.xlabel('Count')
plt.ylabel('Emoji')
plt.title('Top Ten Emojis (Oct 2023)')
plt.show()
```
<html><img src="https://raw.githubusercontent.com/bwegr/386/main/assets/images/topemojis.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width:500px;"/><br><br></html>

#### **Question 3: Who generates the most reactions (total and per message)?**

Let's go a bit deeper and ask what is (perhaps) a more meaningful question: "Who _generates_ the most reactions?. It is one thing to count reactions but reaction _generators_ may function as a proxy for finding those that have influence, or at least those who are creating valuable content. For this, we will go back to our react1a data frame to get info on the user that posted messages and the 'react_count' column that Slack has conveniently provided (so we don't have to extract numbers again from the JSON string in the 'reactions' column. We will be careful though and make sure that we don't just look at total reactions but also reactions per message since we could get a distorted picture with those who have a very small number of messages that happened to have a lot of reactions. This second part will require a bit more minaupltation from our original data frame with all message data to get the denominator of total messages for people. If we wanted to go further (which I don't in this case) we could filter out those who have less than 10 messages to ensure the quality of this metric.

```python
#Top Reaction Generators by Total Reactions
react1b = pd.merge(react1a, users[['id', 'real_name']], left_on='user', right_on = 'id', how='left')
react1b

react1c = react1b.sort_values(by='react_count', ascending=False)
paired = sorted(zip(react1c['react_count'][0:10], react1c['real_name'][0:10]))
count_s, user_s = zip(*paired)
plt.barh(user_s, count_s)
for index, value in enumerate(count_s):
    plt.text(value-35, index, str(value), va='center')
#plt.xlabel('Count')
plt.ylabel('Name')
plt.title('Top Ten Reaction Generators - Total (Oct 2023)')
plt.show()
```
<html><img src="https://raw.githubusercontent.com/bwegr/386/main/assets/images/topreactgen.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width:500px;"/><br><br></html>

```python
# Top Reaction Generators by Reactions per Message
import numpy as np

mess1 = all0[['user','ts']].groupby('user').count().reset_index().sort_values(by='ts', ascending=False).dropna()
mess1.rename(columns={'ts': 'mess_count'}, inplace=True)
react1a.rename(columns={'sum_counts': 'react_count'}, inplace=True)
mess1a = pd.merge(pd.merge(react1a, mess1, on='user', how='left'), users[['id', 'real_name']], left_on='user', right_on = 'id', how='left')
mess1a['re_per_mes'] = round(mess1a['react_count']/mess1a['mess_count'],2)
mess1a = mess1a[['real_name', 're_per_mes']]
mess1a['real_name'] = mess1a['real_name'].astype(str)

mess1b = mess1a.sort_values(by='re_per_mes', ascending=False)
paired = sorted(zip(mess1b['re_per_mes'][0:10], mess1b['real_name'][0:10]))
count_s, user_s = zip(*paired)
print(count_s, user_s)
plt.barh(user_s, count_s)
for index, value in enumerate(count_s):
    plt.text(value-2, index, str(value), va='center')
#plt.xlabel('Count')
plt.ylabel('Name')
plt.title('Top Ten Reaction Generators - Reactions Per Message (Oct 2023)')
plt.show()
```

<html><img src="https://raw.githubusercontent.com/bwegr/386/main/assets/images/topreactgen2.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width:500px;"/><br><br></html>

#### **Question 4: Who replies to messages most?**

Another metric we can look at to measure Slack activity is replies. So who replies to messages most? Whether these people are more helpful than others, more extroverted, or just spend more time on Slack won't be clear, but it may provide an interesting point of comparison to those with the most reactions. I hypothesize that this would be a similar group of people seeing how both are comparable dimensions of slack activity. 

```python

```
<html><img src="https://raw.githubusercontent.com/bwegr/386/main/assets/images/topreply.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width:500px;"/><br><br></html>

#### **Question 5: Who generates the most replies (total and per message)?**

Text Description

```python

```
<html><img src="https://raw.githubusercontent.com/bwegr/386/main/assets/images/topreplygen.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width:500px;"/><br><br></html>

#### **Question 6: Who generates the fastest replies?**

Text Description

```python

```
<html><img src="https://raw.githubusercontent.com/bwegr/386/main/assets/images/topfastreply.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width:500px;"/><br><br></html>

#### **Question 7: What kind of networks exist within channel membership?**

Text Description

```python

```
<html><img src="https://raw.githubusercontent.com/bwegr/386/main/assets/images/topreactors.png" alt="" style="display: block; margin-left: auto; margin-right: auto; width:500px;"/><br><br></html>

```python
import os

def get_folders_in_dir(dir):
    folders = []
    for item in os.listdir(dir):
        full_path = os.path.join(dir, item)
        if os.path.isdir(full_path):
            folders.append(full_path)
    return folders

folders = get_folders_in_dir('slack_data/')
print(folders)
```
```code
['slack_data/2023-internships-rescue-brigade', 'slack_data/2024-cruise', 'slack_data/2024marketers', 'slack_data/2024_southamerica', 'slack_data/2025marketers', 'slack_data/acquisition-info-session-and-competition', 'slack_data/adam-smith-society', 'slack_data/advancedbrandfall23', 'slack_data/anime', 'slack_data/asia_2023_reunion', 'slack_data/b2b_fall_2023', 'slack_data/billion-dollar-contract', 'slack_data/blue_lounge', 'slack_data/byu_game_squad', 'slack_data/case_competitions', 'slack_data/chat', 'slack_data/christine', 'slack_data/christmas-choir-2023', 'slack_data/class-bidding-black-market', 'slack_data/club-blue-forge', 'slack_data/club_analytics', 'slack_data/club_gea', 'slack_data/club_gfa', 'slack_data/club_hla', 'slack_data/club_mbama', 'slack_data/club_mbats', 'slack_data/club_pma', 'slack_data/corporatefinancialreporting-mba501', 'slack_data/course-catalog', 'slack_data/fantasyfootball', 'slack_data/FC_F063PH5B2MA_Type something', 'slack_data/fine_arts', 'slack_data/hay-movers', 'slack_data/housing', 'slack_data/hr2025', 'slack_data/hr_mba548_ta_help', 'slack_data/international2024', 'slack_data/internationals2025', 'slack_data/internationals_mba', 'slack_data/investment-banking', 'slack_data/leadership_mba505_fall23', 'slack_data/lets_mess_with_texas', 'slack_data/lunch', 'slack_data/managerialaccountingmba502', 'slack_data/marketingrecruiting', 'slack_data/marketing_mba550_fall_23', 'slack_data/mba-656a', 'slack_data/mbasketball', 'slack_data/mba_520_ta_help', 'slack_data/mba_moms', 'slack_data/mba_pickleball_tourney', 'slack_data/memes', 'slack_data/national-black-2023', 'slack_data/nw-tech-trip-november2023', 'slack_data/operations_mba530_fall_23', 'slack_data/pickleball', 'slack_data/pro-sems', 'slack_data/quality_mgt', 'slack_data/racquetball', 'slack_data/recruiting', 'slack_data/savage-scholars2024', 'slack_data/sca', 'slack_data/sca-leadership', 'slack_data/scubacrew', 'slack_data/selling_stuff', 'slack_data/service', 'slack_data/shpe-convention-2023', 'slack_data/siliconslopessummitfall2023', 'slack_data/smash-bros', 'slack_data/soccer', 'slack_data/spiritual_strength', 'slack_data/swell_class_winter24', 'slack_data/team-formerly-known-as-the-rats', 'slack_data/techtrek-ben-dal-2023', 'slack_data/temple-trips', 'slack_data/thinktank_fall2023', 'slack_data/tto_info', 'slack_data/vcic-2023', 'slack_data/vcic-byu-internal', 'slack_data/vcic-with-first-years-2023', 'slack_data/volleyball', 'slack_data/women-in-management-wim', 'slack_data/y-hikers']
```
<html><br><br></html>
#### **Step 2: Reading, Combining, and Converting JSON**

Then we will create and use another function that reads in all the JSON files in all the folders, converts them to data frames, and appends them together. This function also adds a column with the date (the name of each separate JSON file) and a column with the name of the channel (the name of the folder that the JSON file is in).


```python
import json
import pandas as pd

def read_json_files(folders):
    all_data = pd.DataFrame()

    for folder in folders:
        for file in os.listdir(folder):
            if file.endswith('.json'):
                file_path = os.path.join(folder, file)
                with open(file_path, 'r') as json_file:
                    data = pd.DataFrame(json.load(json_file))
                    data.insert(0, 'channel', file)
                    all_data.append(data, ignore_index=True)
    return all_data

all = read_json_files(folders)
all
```
<html>
<div style="width: 100%; height: 500px; overflow: auto;">
<style scoped>
    .dataframe {
        font-size: 12px; /* Smaller font size */
    }
    .dataframe td, .dataframe th {
        padding: 4px; /* Smaller padding */
    }
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
    .dataframe tbody tr th {
        vertical-align: top;
    }
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>channel</th>
      <th>date</th>
      <th>client_msg_id</th>
      <th>type</th>
      <th>text</th>
      <th>user</th>
      <th>ts</th>
      <th>blocks</th>
      <th>team</th>
      <th>user_team</th>
      <th>...</th>
      <th>date_delete</th>
      <th>url_private_download</th>
      <th>shares</th>
      <th>inviter</th>
      <th>last_read</th>
      <th>x_files</th>
      <th>root</th>
      <th>username</th>
      <th>app_id</th>
      <th>hidden</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2023-internships-rescue-brigade</td>
      <td>2023-10-18</td>
      <td>A3596BA8-FC44-4B86-8236-C9152409AE8F</td>
      <td>message</td>
      <td>Hola muchachones , los créditos de cada semest...</td>
      <td>U03M8T51VUY</td>
      <td>1.697639e+09</td>
      <td>[{'type': 'rich_text', 'block_id': 'kdJP8', 'e...</td>
      <td>T0ZBAEL59</td>
      <td>T0ZBAEL59</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2023-internships-rescue-brigade</td>
      <td>2023-10-18</td>
      <td>C141977D-A33B-4169-BC8C-22A78F89D3D3</td>
      <td>message</td>
      <td>Es lo que Christine me dijo. Podés preguntarle...</td>
      <td>U03LK4ZPZ4J</td>
      <td>1.697639e+09</td>
      <td>[{'type': 'rich_text', 'block_id': '8jBTM', 'e...</td>
      <td>T0ZBAEL59</td>
      <td>T0ZBAEL59</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2023-internships-rescue-brigade</td>
      <td>2023-10-18</td>
      <td>5C0F4402-3BF3-42AE-A23F-99A34774D4C5</td>
      <td>message</td>
      <td>Buenísimo</td>
      <td>U03M8T51VUY</td>
      <td>1.697640e+09</td>
      <td>[{'type': 'rich_text', 'block_id': 'dh4dY', 'e...</td>
      <td>T0ZBAEL59</td>
      <td>T0ZBAEL59</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2023-internships-rescue-brigade</td>
      <td>2023-10-18</td>
      <td>FF5FB03E-48AD-427A-82AB-43DCFFD8E4C5</td>
      <td>message</td>
      <td>Gracias</td>
      <td>U03M8T51VUY</td>
      <td>1.697640e+09</td>
      <td>[{'type': 'rich_text', 'block_id': '8VnI7', 'e...</td>
      <td>T0ZBAEL59</td>
      <td>T0ZBAEL59</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2023-internships-rescue-brigade</td>
      <td>2023-10-19</td>
      <td>9BE97A04-A58D-45F8-97DB-D377984AE836</td>
      <td>message</td>
      <td>Listo, ya confirme con Christine mis créditos ...</td>
      <td>U03M8T51VUY</td>
      <td>1.697728e+09</td>
      <td>[{'type': 'rich_text', 'block_id': 'X0wyY', 'e...</td>
      <td>T0ZBAEL59</td>
      <td>T0ZBAEL59</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>2743</th>
      <td>y-hikers</td>
      <td>2023-10-13</td>
      <td>2480C1C8-1CE2-4455-B3DB-E72B07A78AB0</td>
      <td>message</td>
      <td>Thanks for the update!</td>
      <td>U03LK4YF3A6</td>
      <td>1.697244e+09</td>
      <td>[{'type': 'rich_text', 'block_id': 'VSUE8', 'e...</td>
      <td>T0ZBAEL59</td>
      <td>T0ZBAEL59</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2744</th>
      <td>y-hikers</td>
      <td>2023-10-13</td>
      <td>002216f5-e7a3-44af-933d-535d5b470c71</td>
      <td>message</td>
      <td>A primer on everyone's favorite oyster idiom...</td>
      <td>U03LK39PKRR</td>
      <td>1.697254e+09</td>
      <td>[{'type': 'rich_text', 'block_id': 'LIV48', 'e...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2745</th>
      <td>y-hikers</td>
      <td>2023-10-13</td>
      <td>EFCEABA8-CC03-456E-9353-DDCEAA0D020A</td>
      <td>message</td>
      <td>Wow. Phenomenal</td>
      <td>U03LK4YBK7C</td>
      <td>1.697259e+09</td>
      <td>[{'type': 'rich_text', 'block_id': 'oqmt0', 'e...</td>
      <td>T0ZBAEL59</td>
      <td>T0ZBAEL59</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2746</th>
      <td>y-hikers</td>
      <td>2023-10-14</td>
      <td>5f70e832-058a-425a-aac8-02124266f89c</td>
      <td>message</td>
      <td>I'm over here reacting to posts with lame yell...</td>
      <td>U03LK39PKRR</td>
      <td>1.697299e+09</td>
      <td>[{'type': 'rich_text', 'block_id': 'SNgyE', 'e...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2747</th>
      <td>y-hikers</td>
      <td>2023-10-14</td>
      <td>00A5F014-AE1D-4C0A-A6AA-194F7A2008D0</td>
      <td>message</td>
      <td>Haha :joy:  we can make all of your dreams com...</td>
      <td>U03LK4YBK7C</td>
      <td>1.697312e+09</td>
      <td>[{'type': 'rich_text', 'block_id': 'lKyVT', 'e...</td>
      <td>T0ZBAEL59</td>
      <td>T0ZBAEL59</td>
      <td>...</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>2748 rows × 53 columns</p>
</div><br><br>
</html>


#### **Step 3: Users and Channels**

We also need to pull in two other JSON files that contain all the users in the slack workspace and all the channels in the slack workspace, respectively. We need the user data because it contains names, images, etc. for each user that we can then join to our message data frame with the user id. We need the channel data frame to answer questions about channel membership (we'll also put the channel members into a list for more streamlined analysis later on).

```python
# Users
upath = "slack_data/users.json"
with open(upath, 'r') as json_file:
    users = pd.read_json(json_file)

# Channel Membership
cpath = "slack_data/channels.json"
with open(cpath, 'r') as json_file:
    chans = pd.read_json(json_file)

chanmem = []
for name, i in zip(chans['name'], chans['members']): chanmem.append(i)

```
<html><br></html>
#### **Conclusion**
Now our data is clean and ready for use! We will do some further manipulation in my next blog post to answer specific questions about the data and see what kinds of insights we can uncover about the BYU MBA program.

#### **Ethical Considerations**
In order to use this data, I obtained permission from the BYU MBA Slack administrators. It is only data from public channels and only from a specific timeframe (October 2023). I will only show data in grouped or summarized form.
